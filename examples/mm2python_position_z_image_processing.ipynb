{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##This workbook describes a multi-position, z-stack acquisition with intermittent image processing\n",
    "The specific example demonstrated incorporates a processing step to assess cell confluency levels  \n",
    "(thanks to Manuel Leonetti, Keith Cheveralls @ czbiohub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from py4j.java_gateway import JavaGateway, GatewayParameters\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage import filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Before running the next cell, be sure you've clicked \"Create Python Bridge\" in the mm2python plugin first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to org.mm2python\n",
    "gateway = JavaGateway(gateway_parameters=GatewayParameters(auto_field=True))\n",
    "\n",
    "# link to micro-manager control\n",
    "gate = gateway\n",
    "ep = gateway.entry_point\n",
    "mm = ep.getStudio()\n",
    "mmc = ep.getCMMCore()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For precise control of micro-manager hardware, please see the documentation here:\n",
    "Studio (mm)\n",
    "Controls the GUI: Multidimensional-acquisition, metadata, display and high-level hardware control\n",
    "- https://valelab4.ucsf.edu/~MM/doc-2.0.0-beta/mmstudio/org/micromanager/Studio.html\n",
    "\n",
    "Core (mmc)\n",
    "Controls the CORE: direct communication with hardware device adapters\n",
    "- https://valelab4.ucsf.edu/~MM/doc-2.0.0-beta/mmcorej/mmcorej/CMMCore.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup your channel names and other hardawre parameters\n",
    "\n",
    "Channels = [\n",
    "    \"EMCCD_Confocal40_DAPI\",\n",
    "    \"EMCCD_Confocal40_GFP\"\n",
    "]\n",
    "\n",
    "Lasers = [\n",
    "    \"Laser 405-Power Setpoint\",\n",
    "    \"Laser 488-Power Setpoint\"\n",
    "]\n",
    "\n",
    "LaserPowers = [\n",
    "    10,\n",
    "    10\n",
    "]\n",
    "\n",
    "Exposures = [ # Must be double\n",
    "    50.0,\n",
    "    50.0\n",
    "]\n",
    "\n",
    "Gain = [ # Must be double\n",
    "    400.0,\n",
    "    400.0\n",
    "]\n",
    "\n",
    "ZStack = [\n",
    "    -6.0,    # Z-stack begin, relative\n",
    "    16.0,   # Z-stack end, relative\n",
    "    0.2     # Z-stack step\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### important note:\n",
    "If py4j complains that \"method is not found\" or similar, it could mean that you are passing the wrong data type.\n",
    "for example:\n",
    "\tmmc.setExposure(100)\t  # will return an error\n",
    "\tmmc.setExposure(100.0)\t  # is ok!\n",
    "python doesn't care about this difference, but java (and thus, micro-manager) does care!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename a few devices to make coding a little easier\n",
    "# adjust the strings according to your hardware\n",
    "\n",
    "zdevice = \"PiezoZ\"\n",
    "xydevice = \"XYStage\"\n",
    "config_group = \"Channels-EMCCD\"\n",
    "camera = \"Andor EMCCD\"\n",
    "laser_line = \"Andor ILE-A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### micro-manager 2.0 uses a concept of \"datastores\" to manage acquired data.\n",
    "see documentation here for more info:\n",
    "- https://micro-manager.org/wiki/Version_2.0_API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JavaObject id=o5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to create a datastore, we need to define a save path:\n",
    "\n",
    "# Save parameters\n",
    "autoSave_path = \"C:/path_for_your_data/\"\n",
    "prefix = \"filename_prefix_here_\"\n",
    "num = 0\n",
    "\n",
    "# check if the file already exists\n",
    "# if it exists, increment num until we have a new file\n",
    "while os.path.isdir(autoSave_path + prefix + str(num)):\n",
    "    num += 1\n",
    "fullpath = autoSave_path + prefix + str(num)\n",
    "\n",
    "# finally, create the datastore!\n",
    "autosavestore = mm.data().createMultipageTIFFDatastore(fullpath, True, True)\n",
    "mm.displays().createDisplay(autosavestore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's set up autofocus using the GUI's AutofocusManager\n",
    "# comment this cell out if you are not using Autofocus!\n",
    "\n",
    "# Set autofocus\n",
    "af_manager = mm.getAutofocusManager()\n",
    "\n",
    "# AFC is Leica's hardware autofocus device.  Rename this appropriately\n",
    "af_manager.setAutofocusMethodByName(\"Adaptive Focus Control\")\n",
    "af_plugin = af_manager.getAutofocusMethod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optionally, you can assign devices to \"ImageSynchro\", \n",
    "# which requires all devices to settle before continuing acquisition\n",
    "mmc.assignImageSynchro(zdevice)\n",
    "mmc.assignImageSynchro(xydevice)\n",
    "mmc.assignImageSynchro(mmc.getShutterDevice())\n",
    "mmc.assignImageSynchro(mmc.getCameraDevice())\n",
    "mmc.setAutoShutter(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### There are two ways one can snap an image in micro-manager:\n",
    "\n",
    "1) using the Studio's SnapLiveManager  \n",
    "2) using the Core's \"snapImage\" method\n",
    "\n",
    "The Studio's SnapLiveManager opens a special window named \"Snap/Live\", takes a snap using current hardware settings\n",
    "and places the image in that window\n",
    "\n",
    "The Core's \"snapImage\" method will take a snap using current hardware settings, but does nothing else.  \n",
    "It is up to the user to assign this data to a datastore.\n",
    "  \n",
    "See examples in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# the camera exposure takes some number of ms to finish.  We have to wait for that to finish\n",
    "# or else \"ep.getLastMeta()\" will return None\n",
    "def wait_for_meta(ep):\n",
    "    ct = 0\n",
    "    meta = ep.getLastMeta()\n",
    "    while not meta:\n",
    "        time.sleep(0.0001)\n",
    "        ct += 1\n",
    "        meta = ep.getLastMeta()\n",
    "        if ct >= 10000:\n",
    "            raise FileExistsError(\"timeout waiting for file exists\")\n",
    "    return meta\n",
    "\n",
    "\n",
    "# Snap using SnapLiveManager\n",
    "# Retrieve data using mm2python's memory mapped file system\n",
    "def get_snap_data(mm, gate):\n",
    "    \n",
    "    # clear out old data from earlier snaps\n",
    "    gate.entry_point.clearQueue()\n",
    "    \n",
    "    # Snaps and writes to snap/live view\n",
    "    mm.live().snap(True)\n",
    "\n",
    "    # Retrieve data from memory mapped files, np.memmap is functionally same as np.array\n",
    "    meta = wait_for_meta(gate.entry_point)\n",
    "    dat = np.memmap(meta.getFilepath(), dtype=\"uint16\", mode='r+', offset=0,\n",
    "                    shape=(meta.getxRange(), meta.getyRange()))\n",
    "\n",
    "    return dat\n",
    "\n",
    "\n",
    "# Snap using Core\n",
    "# This pattern REQUIRES you to define a \n",
    "def get_snap_core(gate, datastore):\n",
    "    mm = gate.entry_point.getStudio()\n",
    "    mmc = gate.entry_point.getCMMCore()\n",
    "    gate.entry_point.clearQueue()\n",
    "    \n",
    "    mmc.snapImage()\n",
    "    tmp1 = mmc.getTaggedImage()\n",
    "    \n",
    "    # Convert tagged image into an Image object\n",
    "    channel0 = mm.data().convertTaggedImage(tmp1)\n",
    "    \n",
    "    # assign metadata (coordinates) to this Image object\n",
    "    channel0 = channel0.copyWith(\n",
    "        channel0.getCoords().copy().channel(c).z(z).stagePosition(p).build(),\n",
    "        channel0.getMetadata().copy().positionName(\"\" + str(p)).build())\n",
    "    \n",
    "    # place this image (with its metadata) in a datastore\n",
    "    # if this datastore is displayed in a window, mm2python will automatically recognize the new data\n",
    "    datastore.putImage(channel0)\n",
    "    \n",
    "    # retrieve the data for python use\n",
    "    # if datastore is not displayed, mm2python won't see new data, then function returns None\n",
    "    try:\n",
    "        meta = wait_for_meta(gate.entry_point)\n",
    "    except FileExistsError:\n",
    "        return None\n",
    "    \n",
    "    dat = np.memmap(meta.getFilepath(), dtype=\"uint16\", mode='r+', offset=0,\n",
    "                    shape=(meta.getxRange(), meta.getyRange()))\n",
    "\n",
    "    return dat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Next we'll define some processing functions to insert into our acquisition\n",
    "The \"overall_confluency\" takes an image region and uses scikit image's filter library to asses cell confluency  \n",
    "The \"spread_test\" splits the image into sub-regions, then passes those to \"overall_confluency\"  \n",
    "\n",
    "\"move_z\" and \"move_z_relative\" are convenience abstractions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Confluency check for image within user-given thresholds\n",
    "def overall_confluency(mid_image, total_pixels, lower_confluence_threshold, upper_confluence_threshold):\n",
    "    # Using only middle slice, apply Gaussian filter\n",
    "    filtered_mid_image = filters.gaussian(mid_image)\n",
    "\n",
    "    # Threshold between background and non-background using Li Thresholding\n",
    "    val = filters.threshold_li(filtered_mid_image)\n",
    "\n",
    "    # Compute percentage of non-background pixels\n",
    "    blue_pixels = len(filtered_mid_image[filtered_mid_image >= val])\n",
    "    blue_pixel_percentage = (blue_pixels / total_pixels) * 100.0\n",
    "    # print(\"Blue pixels:\", str(blue_pixels))\n",
    "    # print(\"Confluency:\", str(blue_pixel_percentage))\n",
    "\n",
    "    if blue_pixel_percentage >= lower_confluence_threshold and \\\n",
    "            blue_pixel_percentage <= upper_confluence_threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Test for good confluency overall and spread of cells\n",
    "def spread_test(slice):\n",
    "    # Set thresholds for making confluency decisions\n",
    "    global_lower_confluence_threshold = 15\n",
    "    global_upper_confluence_threshold = 46\n",
    "    tile_lower_confluence_threshold = 10\n",
    "    tile_upper_confluence_threshold = 50\n",
    "    total_image_pixels = float(1024.0 * 1024.0)\n",
    "    slice_factor = 256\n",
    "    sub_image_pixels = float(slice_factor * slice_factor)\n",
    "    false_counter = 0\n",
    "\n",
    "    if overall_confluency(slice, total_image_pixels, global_lower_confluence_threshold, global_upper_confluence_threshold):\n",
    "        addition_x = 0\n",
    "        addition_y = 0\n",
    "\n",
    "        # Splitting image into 16 256 x 256 tiles and testing confluency on each tile\n",
    "        for n in range(16):\n",
    "            sub_image = np.empty((slice_factor, slice_factor))\n",
    "            for i in range(slice_factor):\n",
    "                for j in range(slice_factor):\n",
    "                    sub_image[i][j] = slice[i + addition_x][j + addition_y]\n",
    "\n",
    "            # Actual confluency check for single tile\n",
    "            sub_image_confluency = overall_confluency(sub_image, sub_image_pixels, tile_lower_confluence_threshold, tile_upper_confluence_threshold)\n",
    "            if sub_image_confluency == False:\n",
    "                false_counter += 1\n",
    "\n",
    "            # Updates values for tiling\n",
    "            if n == 3 or n == 7 or n == 11:\n",
    "                addition_y += slice_factor\n",
    "                addition_x = 0\n",
    "            else:\n",
    "                addition_x += slice_factor\n",
    "\n",
    "        if false_counter > 4:\n",
    "            print(\"Bad spread\")\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        print(\"Overall confluency fail\")\n",
    "        return False\n",
    "    \n",
    "\n",
    "# Quick command to move FocusDrive to absolute position\n",
    "def move_z(mmc, zdevice, newZ):\n",
    "    mmc.setPosition(zdevice, newZ)\n",
    "    mmc.waitForDevice(zdevice)\n",
    "    curPos = mmc.getPosition(zdevice)\n",
    "    return curPos\n",
    "\n",
    "\n",
    "# Quick command to move FocusDrive relative to the current position\n",
    "def move_z_relative(mmc, zdevice, offset):\n",
    "    mmc.setRelativePosition(zdevice, offset)\n",
    "    mmc.waitForDevice(zdevice)\n",
    "    curPos = mmc.getPosition(zdevice)\n",
    "    return curPos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup is done, now we can run the acquisition loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pl = mm.getPositionList()\n",
    "\n",
    "# Position loop\n",
    "for p in range(pl.getNumberOfPositions()):\n",
    "    \n",
    "    # Reset PiezoZ\n",
    "    mmc.setPosition(zdevice, 0.0)\n",
    "\n",
    "    # Move to next position\n",
    "    nextPosition = pl.getPosition(p)\n",
    "    nextPosition.goToPosition(nextPosition, mmc)\n",
    "    \n",
    "    # Autofocus\n",
    "    print(\"Focusing...\")\n",
    "    mmc.setConfig(config_group, Channels[0])\n",
    "    mmc.waitForConfig(config_group, Channels[0])\n",
    "    mmc.setExposure(float(Exposures[0]))\n",
    "    mmc.setProperty(laser_line, Lasers[0], LaserPowers[0])\n",
    "    mmc.setProperty(camera, \"Gain\", Gain[0])\n",
    "    try:\n",
    "        af_plugin.fullFocus()\n",
    "    except:\n",
    "        print(\"AFC FAIL\")\n",
    "    mmc.waitForSystem()\n",
    "    focalPlane = mmc.getPosition(zdevice)\n",
    "    curPos = focalPlane\n",
    "    print(\"Found focus.\")\n",
    "    mmc.waitForSystem()\n",
    "\n",
    "    # spread_test is a function that evaluates cell density (confluency)\n",
    "    if spread_test(get_snap_data(mm, gate)):\n",
    "        print(\"Good confluency\")\n",
    "\n",
    "        # Move to bottom of stack\n",
    "        print(\"Moving the stage to: \" + str(ZStack[0]))\n",
    "        floor = move_z_relative(mmc, zdevice, ZStack[0])\n",
    "\n",
    "        # Channel loop\n",
    "        for c in range(len(Channels)):\n",
    "            print(\"Now imaging:\" + Channels[c])\n",
    "            mmc.setConfig(config_group, Channels[c])\n",
    "            mmc.setProperty(laser_line, Lasers[c], LaserPowers[c])\n",
    "            mmc.setExposure(float(Exposures[c]))\n",
    "            mmc.setProperty(camera, \"Gain\", Gain[c])\n",
    "            z = 0\n",
    "            curPos = mmc.getPosition(zdevice)\n",
    "\n",
    "            # Z-position loop\n",
    "            while curPos <= focalPlane + ZStack[1]:\n",
    "                \n",
    "                # acquire data\n",
    "                mmc.waitForImageSynchro()\n",
    "                get_snap_core(gate, autosavestore)\n",
    "                \n",
    "                # Move to next z-position\n",
    "                curPos = move_z_relative(mmc, zdevice, ZStack[2])\n",
    "                z += 1\n",
    "\n",
    "            move_z(mmc, zdevice, floor)\n",
    "\n",
    "        move_z(mmc, zdevice, focalPlane)\n",
    "\n",
    "    else: # Confluency at this position is poor, moving to next position\n",
    "        print(\"Poor confluency, skipping position.\")\n",
    "\n",
    "autosavestore.freeze()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
